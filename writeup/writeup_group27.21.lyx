#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 503
\begin_document
\begin_header
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Can we (artificially) understand Seinfeld's humor?
\end_layout

\begin_layout Author
Dan Kufra (30219082-2 dank) [
\begin_inset CommandInset href
LatexCommand href
target "dan.kufra@mail.huji.ac.il"
type "mailto:"

\end_inset

]
\begin_inset Newline newline
\end_inset

Yuval Jacoby (30224707-7 yuvalja) [
\begin_inset CommandInset href
LatexCommand href
target "yuval.jacoby@mail.huji.ac.il"
type "mailto:"

\end_inset

]
\begin_inset Newline newline
\end_inset

Alon Netser (31160253-6, alonnetser) [
\begin_inset CommandInset href
LatexCommand href
target "alon.netser@mail.huji.ac.il"
type "mailto:"

\end_inset

]
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract
Who among us isn't familiar with the hilarious sitcom 
\begin_inset Quotes eld
\end_inset

Seinfeld
\begin_inset Quotes erd
\end_inset

? We've all laughed from Jerry's stand-up scenes (
\begin_inset CommandInset href
LatexCommand href
name "YouTube"
target "https://www.youtube.com/watch?v=Qe9YzGYDJk8"

\end_inset

), George Costanza's ridiculous behavior (
\begin_inset CommandInset href
LatexCommand href
name "YouTube"
target "https://www.youtube.com/watch?v=XoMx_5KuUkI"

\end_inset

), Kramer's facial expressions and bursting into the room (
\begin_inset CommandInset href
LatexCommand href
name "YouTube"
target "https://www.youtube.com/watch?v=epmvG3pjpu0"

\end_inset

), and Elaine pushing everyone away and shouting 
\begin_inset Quotes eld
\end_inset

Get Out!
\begin_inset Quotes erd
\end_inset

 (
\begin_inset CommandInset href
LatexCommand href
name "YouTube"
target "https://www.youtube.com/watch?v=JBgcgQD5bJs"

\end_inset

).
\end_layout

\begin_layout Abstract
The main question we ask is - can we build a model that understands this
 humor as we do?
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Problem Description
\end_layout

\begin_layout Standard
We want to build a model that 
\begin_inset Quotes eld
\end_inset

understands humor
\begin_inset Quotes erd
\end_inset

, i.e.
 given the text of a scene from Seinfeld the model will predict the 
\begin_inset Quotes eld
\end_inset

funniness
\begin_inset Quotes erd
\end_inset

 of each sentence.
 First, we validate the data.
 We find certain flaws and inaccuracies, and think about how to deal with
 them.
 Second, we analyze the data and visualize its content.
 Third, we visualize the connections between the different characters in
 the show, and build an interactive graph describing these relations.
 And finally we train various prediction models that attempt to learn this
 task and analyze their results.
\end_layout

\begin_layout Subsection
Difficulties
\end_layout

\begin_layout Standard
The types of problems we are addressing are very difficult, we will attempt
 to break them into groups:
\end_layout

\begin_layout Subsubsection
Computational Humor detector
\end_layout

\begin_layout Standard
We use humor all the time, but it's still hard to explain why something
 is funny.
 Computational understanding of humor is even harder.
 There are plenty of works in this field; for example 
\begin_inset CommandInset citation
LatexCommand cite
key "key-2,key-3,key-6"

\end_inset

.
 One of the problems with detecting humor is the subjectiveness of the term
 
\begin_inset Quotes eld
\end_inset

funny
\begin_inset Quotes erd
\end_inset

 i.e.
 the absence of ground truth.
 In this task, we will try to predict the Seinfeld writer's humor, in this
 case we do have labels, thought this work we use funny as funny be Seinfeld
 writers.
\end_layout

\begin_layout Subsubsection
Understanding Text
\end_layout

\begin_layout Standard
In order to understand humor in text we need to understand the text where
 we have ambiguity of words, dynamic language etc..
 Moreover, context and timing of text are really important for understanding
 humor.
 When we watch an episode of Seinfeld we understand the overall topic, and
 remember events that occurred previously in the episode (and even in other
 episodes).
 In order to make the computer understand the humor as well, we must use
 a model that is capable of understanding context and 
\begin_inset Quotes eld
\end_inset

remembering
\begin_inset Quotes erd
\end_inset

 important events from the past.
\end_layout

\begin_layout Subsubsection
Funny is not taken only from the text
\end_layout

\begin_layout Standard
When we watch Seinfeld and laugh, it is not only because of the text.
 Most of the times it is affected by the intonation, and more generally
 the whole scene's video.
 For example when Kramer enters the room demonstratively 
\begin_inset Quotes eld
\end_inset

in a funny way
\begin_inset Quotes erd
\end_inset

.
 These aspects are not always obvious in the text itself.
 So already there is a limit to the accuracy that can be achieved from a
 purely textual attempt.
 Regardless there have been some other works that have attempted to predict
 funninesses in sitcoms, such as 
\begin_inset Quotes eld
\end_inset

The Big Bang Theory
\begin_inset Quotes erd
\end_inset

, using text
\begin_inset CommandInset citation
LatexCommand cite
key "key-4"

\end_inset

.
\end_layout

\begin_layout Section
Data
\end_layout

\begin_layout Subsection
Introduction
\end_layout

\begin_layout Standard
The data we use is basically Seinfeld's subtitles, coupled with the speaking
 character and timing - of the sentences said and the laugh-tracks (if funny).
 We downloaded the dataset from 
\begin_inset CommandInset href
LatexCommand href
name "GitHub"
target "https://github.com/ranyadshalom/seinfeld_laugh_corpus"

\end_inset

, and we thank 
\begin_inset CommandInset href
LatexCommand href
name "Ran Yad Shalom"
target "ranyadshalom@gmail.com"
type "mailto:"

\end_inset

 and 
\begin_inset CommandInset href
LatexCommand href
name "Yoav Golberg"
target "https://www.cs.bgu.ac.il/~yoavg/uni/"

\end_inset

 for the great dataset they built 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset

.
\end_layout

\begin_layout Subsection
Specs
\end_layout

\begin_layout Standard
The dataset contains 96 humor annotated “Seinfeld” screenplays, along with
 the timing of the laughter and the timing of the dialog.
 R.
 Y.
 Shalom and Y.
 Golberg got the subtitles from 
\begin_inset CommandInset href
LatexCommand href
target "opensubtitles.org"

\end_inset

, the scripts from 
\begin_inset CommandInset href
LatexCommand href
name "seinology.com"
target "http://www.seinology.com/scripts-english.shtml"

\end_inset

 and used the audio tracks to extract the exact timing of the laugh-tracks.
 Furthermore, they used quite sophisticated techniques to align the subtitles
 with the exact timing, and attach the speaking character for every sentence
 using the scripts.
 You can read about it further in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset

.
 Due to the technique they used to build the data (using the fact that the
 dialogs were recorded in mono, and the laugh-track in stereo) we have the
 episodes starting at season 4 episode 6.
 This is because the previous episodes were not recorded this way.
\begin_inset Newline newline
\end_inset

There are 46,497 sentences in total, associated with several properties:
\end_layout

\begin_layout Itemize
'character': The speaking character
\end_layout

\begin_layout Itemize
'txt': The text
\end_layout

\begin_layout Itemize
'start': Start time (in seconds)
\end_layout

\begin_layout Itemize
'end': End time (in seconds)
\end_layout

\begin_layout Itemize
'is_funny': Whether a laugh occurred after this sentence or not
\end_layout

\begin_layout Itemize
'laugh_time': If this sentence is funny, this is the timing of the laugh
 (in seconds).
 Note that if the sentence was not funny, this is set to NaN
\end_layout

\begin_layout Itemize
Various meta-data about the episode, such as 'season' (season's number),
 'episode_num', 'episode_name', 'total_lines', 'global_episode_num' (in
 the whole dataset)
\end_layout

\begin_layout Itemize
Useful features of the sentence, such as 'num_words', 'length' (in seconds),
 'line_num' (in the episode), 'avg_word_length' (in letters)
\end_layout

\begin_layout Standard
For example, here are 3 lines from our dataset (meta-data such as season,
 episode, etc omitted).
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size scriptsize
character
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size scriptsize
text
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size scriptsize
start
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size scriptsize
end
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size scriptsize
is_funny
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\size scriptsize
laugh_time
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
SUSAN
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
I told you to take the offer.
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
199.003
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
201.469
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
F
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
GEORGE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
Look, I had nothing to do with this.
 It wasn't my decision.
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
201.539
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
205.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
F
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
GEORGE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
It was Jerry.
 Jerry told me.
  I'm the creative guy.
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
206.044
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
209.341
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
T
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\size scriptsize
208.3
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Out of the 
\begin_inset Formula $46,497$
\end_inset

 there are 
\begin_inset Formula $\sim30\%$
\end_inset

 of funny sentences (
\begin_inset Formula $13,560$
\end_inset

).
\end_layout

\begin_layout Subsection
Data Validation - no dataset is perfect...
\end_layout

\begin_layout Remark*
This section was done in an interactive Jupyter Notebook named 'Data_Cleaning.ipy
nb'.
 You are more than welcome to take a look!
\end_layout

\begin_layout Standard
The first task was analyzing the dataset and validating it.
 We watched several episodes and looked at the dataset at the same time.
 We saw that the text is pretty accurate, except minor glitches such as,
 "You met her in the supermarket.
 How did you do that?" was shortened to "You met her in the supermarket.
 How?".
 
\begin_inset Newline newline
\end_inset

The timing of the talking are also pretty accurate, and by reading the paper
 of Ran Yad-Shalom 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset

 who created this dataset he addressed this issue specifically and payed
 extra attention to take several subtitles and choose the one that is best
 aligned with the audio.
\end_layout

\begin_layout Subsubsection
The first flaw: the laugh-tracks
\end_layout

\begin_layout Standard
The laugh track is in the middle of a sentence and sometimes it is during
 multiple sentences.
 We treat a sentence as funny if there was a laughter during the sentence
 (sentence start <= laugh time <= sentence end).
\end_layout

\begin_layout Subsubsection
The second flaw: the speaking character is sometime mislabeled.
\end_layout

\begin_layout Standard
While visualizing the data we saw a weird phenomenon (fig 1), the histogram
 of the amount of sentences in a row has a very long tail, with up to 31
 sentences in a row.
 While there are scenarios where this is possible (a phone call where we
 hear only one side, or a long monologue), we decided to further investigate
 and watched multiple scenes where this happens, while doing so we did see
 that in many situations this is a mistake in the speaking character tag
 of the data.
 
\begin_inset Newline newline
\end_inset

We collected all these cases, and analyzed them statistically.
 We saw that the mean sentences-in-a-row is around 1, and that 99% of the
 times it is below 8.
 We went over the cases where the amount of sentences said by a character
 in a row is more than 8 and manually fixed some of them.
 However, we did see that many of them are actual long monologues by one
 character so we decided to keep them in the data.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename repeating_characters_hist.png
	lyxscale 25
	scale 25

\end_inset

 
\begin_inset Graphics
	filename repeating_characters_hist_tail.png
	lyxscale 25
	scale 25

\end_inset


\begin_inset Newline newline
\end_inset

figure 1 - (Left) Number of sentences in-a-row.
\begin_inset Newline newline
\end_inset

(Right) Number of sentences in-a-row, bigger then 10 (the 
\begin_inset Quotes eld
\end_inset

tail
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Subsubsection
Splitting the sentences into scenes
\end_layout

\begin_layout Standard
Our intuition was that information within a scene is more relevant to other
 sentences in that scene.
 So we attempted to split our data into scenes (by looking at average time
 between sentences and character changes).
 Unfortunately, the data was too noisy and we were not able to split it
 up to an adequate level.
 Our attempts can be seen in the 'Scene.ipynb' ipython notebook.
\end_layout

\begin_layout Subsection
Data Visualizations
\end_layout

\begin_layout Remark*
This section was done in an interactive Jupyter Notebook named 'Visualizations.ip
ynb'.
 You are more than welcome to take a look!
\end_layout

\begin_layout Standard
We tried to visualize several aspects of the data, in order to get more
 insight about it and get ideas about how to solve it.
\end_layout

\begin_layout Subsubsection
Finding Meaningful Features for a Sentence
\end_layout

\begin_layout Standard
We wanted to see if there are differences between funny and not-funny sentences
 in several aspects, such as length (in seconds), number of words in the
 sentence, speech-rate (words per second), etc.
 We saw that the distributions of the length / #words is slightly different
 between funny and not-funny sentences.
 However, these differences are not significant, as the standard deviations
 are also pretty high.
 We found that the length in seconds and the length in #word behave different,
 and the speech-rate seems also like a good feature (see the figure below).
\begin_inset Newline newline
\end_inset

Our conclusions were to add these features to the sentence.
 We saw that the results improved when we gave them these features.
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename elaine_rate_histograms.png
	lyxscale 20
	scale 20

\end_inset


\begin_inset Newline newline
\end_inset

figure 2 - Elaine's speech-rate
\end_layout

\begin_layout Subsubsection
Word-Clouds
\end_layout

\begin_layout Standard
Intuitively, each character tends to use different vocabulary, we tried
 to see if that also holds for funny/not funny sentences.
 To do so we removed very frequent English words (using the 
\begin_inset CommandInset href
LatexCommand href
name "gensim"
target "https://radimrehurek.com/gensim/"

\end_inset

 corpus).
\begin_inset Newline newline
\end_inset

First, we found that the main characters have a variety of words which they
 all use, for example: 
\shape italic
yeah, oh.
\shape default

\begin_inset Newline newline
\end_inset

And also a couple of distinct words such as: Jerry - 
\shape italic
car
\shape default
, Kramer - 
\shape italic
Newman.
\shape default

\begin_inset Newline newline
\end_inset

Further, we wanted to see if characters use different words in funny and
 non funny sentences.
\begin_inset Newline newline
\end_inset

In order to get significant words, we filtered out words that are common
 in both types i.e.
 we used only words that: 
\begin_inset Formula $\frac{\#funny}{\#total}>0.5$
\end_inset

, in figure-3 we can see Jerry's word clouds, for example when Jerry talks
 about dating it's usually funny.
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename Jerry_funny_wordcloud.png
	lyxscale 20
	scale 50

\end_inset


\begin_inset Newline newline
\end_inset

figure 3 - Jerry's Word Cloud
\end_layout

\begin_layout Subsubsection
Characters Graph
\end_layout

\begin_layout Standard
Another nice visualization was the characters-graph, containing the different
 characters and their connections.
 Each edge's weight is proportional to the amount of times the characters
 spoke to each other.
 One can see perfectly the 
\begin_inset Quotes eld
\end_inset

community
\begin_inset Quotes erd
\end_inset

 of the 4 main characters (Jerry, George, Kramer and Elaine), as well as
 other secondary characters (such as Jerry's parents Morty and Helen), this
 graph is interactive and for full experience please open the notebook.
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename charcters_connections.png

\end_inset


\begin_inset Newline newline
\end_inset

figure 4 - Seinfeld characters network graph
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Standard
In our attempt to succeed at this prediction task we implemented a number
 models.
 These can be split into bag-of-word type models and sequence type models.
\end_layout

\begin_layout Subsection
Bag-Of-Words models
\end_layout

\begin_layout Standard
In this type of model we represent each sentence as a one-hot encoding of
 the sentence in a vector the length of all the words in our corpus.
 For example, for the sentence 
\begin_inset Quotes eld
\end_inset

He's a bubble boy!
\begin_inset Quotes erd
\end_inset

 we would have an 1 in the corresponding index of each word.
 For the sentence 
\begin_inset Quotes eld
\end_inset

Yada yada yada
\begin_inset Quotes erd
\end_inset

 we would have the index corresponding to the word 
\begin_inset Quotes eld
\end_inset

Yada
\begin_inset Quotes erd
\end_inset

 be equal to 1.
 These types of models do not preserve order of the words and as such we
 did not expect them to be able to learn much.
 Nonetheless, we implemented both logistic regression (with binary bag of
 words) and a multi-layer perceptron model (with tf-idf bag of words and
 ngrams).
\end_layout

\begin_layout Subsection
Sequence models
\end_layout

\begin_layout Standard
In this type of model we represent each sentence as a series of words, where
 each word is a one-hot encoding of that word in a vector the length of
 all the words (as in the bag-of words model).
 The difference is that we 
\series bold
do 
\series default
preserve the order of the words.
 We felt these type of models would more accurately represent our task since
 we could not find any major correlation between funniness and certain words.
\begin_inset Newline newline
\end_inset

In addition, after our initial data analysis we saw that many of the high-level
 features of the sentence can be indicative of its 
\begin_inset Quotes eld
\end_inset

funniness
\begin_inset Quotes erd
\end_inset

.
 So in each of these models we also created a version where the following
 features are inputted at some stage:
\end_layout

\begin_layout Itemize
An encoding of which character is speaking
\end_layout

\begin_layout Itemize
The start time of the sentence.
\end_layout

\begin_layout Itemize
The length of the sentence in time.
\end_layout

\begin_layout Itemize
The amount of words in the sentence.
\end_layout

\begin_layout Itemize
The rate of speaking (length/number of words).
\end_layout

\begin_layout Itemize
The average word length of the sentence.
\end_layout

\begin_layout Standard
The first two models implemented worked per sentence.
 Meaning they did not use information from sentences that are from the same
 scene/episode and may be relevant.
 These models are:
\end_layout

\begin_layout Subsubsection
Long Short Term Memory model:
\end_layout

\begin_layout Standard
Given a sentence each word is put through a Glove embedding layer to a dimension
 of 100.
 These embeddings are then put through a Bidirectional LSTM with an output
 of 128 units.
 We use an attention mechanism on the series of outputs.
 We then either concatenated the high level features or didn't (depending
 on the experiment).
 Then after a few more Fully Connected layers we output a measure of funniness.
 
\begin_inset Newline newline
\end_inset

We also added another output directly after the attention mechanism (before
 the concatenation with high level features) in order to ensure the loss
 propogates in some way back to the LSTM cells.
 Our final loss is a weighted binary cross-entropy loss of these two outputs.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename LSTM_Graph.png
	scale 50

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\align center
figure 5 - Long Short Term Memory model's graph
\end_layout

\begin_layout Subsubsection
Convolutional Neural Network model:
\end_layout

\begin_layout Standard
Given a sentence each word is put through a Glove embedding layer to a dimension
 of 100.
 This is then passed through 3 CNN blocks of the following:
\end_layout

\begin_layout Itemize
Seperable 1D Convolution
\end_layout

\begin_layout Itemize
Seperable 1D Convolution
\end_layout

\begin_layout Itemize
1D Max Pooling
\end_layout

\begin_layout Standard
Then a 1 dimensional Average Pooling layer and a Fully Connected layer.
 Then depending on the experiment, a concatenation of the high level features
 and another Fully Connected layer for the output.
 We then apply a binary cross-entropy loss on the two outputs.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename CNN_graph.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\align center
figure 6 - Convolutional Neural Network model's graph
\end_layout

\begin_layout Standard
The final model we implemented did make use of the additonal information
 of other relevant sentences:
\end_layout

\begin_layout Subsubsection
Long Short Term Memory Multi-Sentence model:
\end_layout

\begin_layout Standard
In this model each example is actually a series of sentences in an episode.
 Each sentence on its own is represented in the same way as in the previous
 two models.
 We then pass each sentence through a CNN model as described before.
 But then instead of predicting one output for each sentence, we pass all
 the sentences (after the convolution model) through a Bidirectional LSTM.
 We then concatenated each sentence's high level features and go through
 a final Fully Connected layer before predicting the 
\begin_inset Quotes eld
\end_inset

funniness
\begin_inset Quotes erd
\end_inset

 per sentence and applying a weighted binary cross-entropy loss on the two
 outputs.
\end_layout

\begin_layout Standard
\align center

\series bold
\emph on
\begin_inset Graphics
	filename Multi_sentence_lstm_graph.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\align center
figure 7 - Multi Sentence LSTM model's graph
\end_layout

\begin_layout Section
Examining the Results
\end_layout

\begin_layout Remark*
This section was done in an interactive Jupyter Notebook named 'Analyze_Resulsts.
ipynb'.
 You are more than welcome to take a look!
\end_layout

\begin_layout Standard
After we finished training the different models we built, we turned to analyze
 their performance.
 We looked visually at some of the results, and plotted several measures.
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename ROC_curves.png
	lyxscale 70
	scale 70

\end_inset


\begin_inset Newline newline
\end_inset

figure 8 - ROC-curves
\end_layout

\begin_layout Standard
\noindent
We saw that the features we added helped the models greatly.
 Here are the scores for the CNN and LSTM models, with/without the additional
 features.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename CNN_and_LSTM_no_ftrs.png
	lyxscale 30
	scale 30

\end_inset


\begin_inset Graphics
	filename CNN_and_LSTM.png
	lyxscale 30
	scale 30

\end_inset


\begin_inset Newline newline
\end_inset

figure 9 - Scores with/without the additional features.
\end_layout

\begin_layout Standard
\noindent
We also attach the confusion matrices of the predictions of our various
 models, showing a clear improvement from Bag Of Word models to Sequence
 models, and from Sequence models without high level features to those with.
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename confusion_matrices.png
	lyxscale 30
	scale 35

\end_inset


\begin_inset Newline newline
\end_inset

figure 10 - Confusion matrices comparison
\end_layout

\begin_layout Standard
\noindent
It's not perfect, but it definitely learned something.
 We must remember that our data is quite noisy, so there is a limit on the
 accuracy these model can achieve.
\end_layout

\begin_layout Standard
\noindent
We tried to split the 4 main characters and see if the performances are
 different when we look at certain character.
 
\end_layout

\begin_layout Standard
\noindent
We found that there are minor changes, but overall they're all quite the
 same.
 You can see for yourself in the notebook.
\end_layout

\begin_layout Standard
\noindent
Furthermore, we wanted to look at some of the results visually.
\end_layout

\begin_layout Standard
\noindent
Here is the most severe false-positive (i.e.
 the model thought it's funny but it doesn't).
 We also add some context (5 sentences before the allegedly 'funny' sentence).
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename most_severa_false_positive.png
	lyxscale 30
	scale 30

\end_inset


\begin_inset Newline newline
\end_inset

figure 11 - example of 
\begin_inset Quotes eld
\end_inset

Severe
\begin_inset Quotes erd
\end_inset

 FP
\end_layout

\begin_layout Standard
\noindent
Here are the severe false-negatives (i.e.
 the model thought it is not funny but it is):
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename severe_false_negatives.png
	lyxscale 30
	scale 30

\end_inset


\begin_inset Newline newline
\end_inset

figure 12 - example of 
\begin_inset Quotes eld
\end_inset

Severe
\begin_inset Quotes erd
\end_inset

 FN
\end_layout

\begin_layout Standard
\noindent
These mistakes don't seem that severe.
 By looking at it (without checking the label) one can think its label is
 the opposite.
 It demonstrates how difficult this task is, how noisy the data is, and
 can explain why the Multi Sentence LSTM model did not give a large increase
 in accuracy.
 If we as humans find it hard to decide which specific sentence in a scene
 is a funny one, then it must be hard for our model.
 
\end_layout

\begin_layout Section
Conclusion and Future Work
\end_layout

\begin_layout Standard
Given an annotated dataset of the laugh track in Seinfeld subtitles we implement
ed multiple models that predict whether or not a sentence from seinfeld
 is funny based on both textual and high level features.
 We did this using both Bag Of Word and Sequence type models and showed
 a clear advantage to the latter type.
 In addition, we showed that the additional features such as the speaker,
 rate of speaking and length are very indicitave of when there is laughter.
 
\begin_inset Newline newline
\end_inset

While the data for the most part was accurate, we did find some issues that
 would be worth addressing in any future work.
 One of these issues was that many times sentences are funny or not based
 on factors other than text.
 Therefore, a nice work that could be attempted is inputting features of
 the intonation of a sentence (either through audio input or other annotation).
 
\begin_inset Newline newline
\end_inset

It would also be interesting to use these methods on a combination of sitcoms
 and see whether we can generalize to other untrained sitcoms.
 One way to do so is to represent each character in the sitcom as some mixture
 of type-cast sitcom characters and then attempt to generalize between sitcoms
 based on that representation.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

 R.
 Yad-Shalom and Y.
 Goldberg, The Seinfeld Corpus: A Method for Generating A Humor Annotated
 Corpus and An Analysis of That Corpus.
 Computer Science Department, Bar-Ilan University, Israel, 2017.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

R.
 Mihalcea and S.
 Pulman.
 Characterizing humour: An exploration of features in humorous texts.
 In Computational Linguistics and Intelligent Text Processing, pages 337–347.
 Springer, 2007
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

 D.
 Shahaf, E.
 Horvitz, and R.
 Mankoff.
 Inside jokes: Identifying humorous cartoon captions.
 In SIGKDD International Conference on Knowledge Discovery and Data Mining,
 2015.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset

 Bertero, D., Fung, P.: A long short-term memory framework for predicting
 humor in dialogues.
 In: Proceedings of the 2016 Conference of the North American Chapter of
 the Association for Computational Linguistics: Human Language Technologies
 (2016)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"

\end_inset

Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak Bhattacharyya, and
 Mark Carman.
 2016.
 Are word embedding-based features useful for sarcasm detection? In Conference
 on Empirical Methods in Natural Language Processing (EMNLP)
\end_layout

\end_body
\end_document
